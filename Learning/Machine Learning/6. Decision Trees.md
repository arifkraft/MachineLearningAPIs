# Decision Trees
In this chapter we will start by discussing how to train, visualize, and make predictions with Decision
Trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will discuss
how to regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations
of Decision Trees.

## Training and Visualizing Decision Trees
Lets first build a decision tree model and see how that unfolds:
```python
from sklearn.datasets import load_iris 
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:,2:] # petal length and width as predictors 
Y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X,Y)
```
We visualize our model unsing `export_graphviz` as shown below:
```python
from sklearn.tree import export_graphviz
export_graphviz(tree_clf,
                out_file="iris_tree.dot", # In your home directory
                feature_names=iris.feature_names[2:],
                class_names=iris.target_names,
                rounded=True,
                filled=True
                )
```
`.dot` can be converted into different formats such as pdf and png
```
$ dot -Tpng iris_tree.dot -o iris_tree.png
```
Then you can convert this .dot file to a variety of formats such as PDF or PNG using the dot commandline
tool from the `graphviz package` This command line converts the `.dot` file to a `.png` image file:
<br>
*You can expect to see something like this:*

<img src="https://amunategui.github.io/simple-heuristics/img/iris-two-deep.png" width=450>

## Making Predictions 
Suppose you find an iris flower and wish to classify it -
<br>
1. Start at the root node (depth = 0) {is the petal_width <= 0.8}
2. If yes, go to child node (depth = 1) {classify the flower as `setosa`}
Pretty sef-explanatory. isn't it?

> One useful thing to node: Decision Trees is that they require very little data preparation. In particular, they don’t require
feature scaling or centering at all.

Lets have a look at some node-characteristics:
1. **samples** counts no. of `training_instances` in a node. For example, `Node(depth =1, left)` has 50 training_instances after `split` from the `parent node`.
2. **value** tells you how many training instances of
each class this node applies to. Example, the bottom right node has 0 setosa, 1 versicolor and 45 virginica. Ideally you'd expect all 
50 instances of virginica class fall under this node
3. **gini** attribute measures its impurity: a node is “pure”
(gini=0) if all training instances it applies to belong to the same class. For example, the depth-2 left
node has a gini score equal to 1 – (0/54)^2 – (49/54)^2 – (5/54)^2 ≈ 0.168.

<img src="http://www.learnbymarketing.com/wp-content/uploads/2016/02/gini-index-formula.png" width=250>

> Note: Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children (i.e.,
questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have
more than two children.

## Estimating Class Probabilities
Suppose that you are trying to predict the class of a random iris flower. After traversing through the tree it falls into the leaf node
`Node(depth=2, left)` then, the predicted class probabilities are: 
1. **setosa**      : 0/54  = 0%
2. **versicolor**  : 49/54 = **90.7%**
3. **virginica**   : 5/54  = 9.3%

Hence the prediction for the flower is: `versicolor`
```python
tree_clf.predict_proba([[1.2,0.3]])
tree_clf.predict([[5, 1.5]])
```
## The CART Algorithm
The idea is really quite simple: the algorithm first splits the training set in two
subsets using a single feature `k` and a threshold `tk` (e.g., “`petal length ≤ 0.8 cm`”). How does it choose k
and tk? It searches for the pair (k, tk) that produces the `purest` subsets (weighted by their size). The `cost
function` that the algorithm tries to minimize is given by:

**TO INSERT IMAGE**

Once it has successfully split the training set in two, it splits the subsets using the same logic, then the subsubsets
and so on, recursively. It stops recursing once it reaches the maximum `depth` (defined by the
`max_depth` hyperparameter), or if it cannot find a split that will reduce `impurity`. A few other
hyperparameters (described in a moment) control additional stopping conditions (`min_samples_split,
min_samples_leaf, min_weight_fraction_leaf, and max_leaf_nodes`).

> WARNING: CART is a greedy algorithm. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution. finding the optimal tree is known to be an NP-Complete problem:2 it requires O(exp(m))
time, making the problem intractable even for fairly small training sets. This is why we must settle for a
“reasonably good” solution.

## Gini or Impurity?
Gini is used by default, however, there is another impurity function called `entropy`. In Machine Learning, it is frequently used as an impurity measure: a set’s entropy is zero when
it contains instances of only one class.

**TO INSERT IMAGE**

So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big
difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default.
However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the
tree, while entropy tends to produce slightly more balanced trees.

## Regularization Hyperparameters 

Decision Trees make very few assumptions about the training data (as opposed to linear models, which
obviously assume that the data is linear, for example). If left unconstrained, the tree structure will adapt
itself to the training data, fitting it very closely, and most likely overfitting it. Such a model is often called
a nonparametric model, not because it does not have any parameters (it often has a lot) but because the
number of parameters is not determined prior to training, so the model structure is free to stick closely to
the data. In contrast, a parametric model such as a linear model has a predetermined number of
parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of
underfitting).
<br>
Some `hyperparameters` that can help reduce overfitting!
1. **max_depth**: Restrict the maximum depth of the Decision Tree. By default it is `None` i.e. unlimited
1. **min_sample_split**: The minimum number of samples a node must have before it can be split
1. **min_samples_leaf**: The minimum number of samples a leaf node must have
1. **min_weight_fraction_leaf**: Same as min_samples_leaf but expressed as a fraction of the total
number of weighted instances
1. **max_leaf_nodes**: maximum number of leaf nodes
1. **max_features**: maximum number of features that are evaluated for splitting at each node

> Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.

## Regression Trees
Decision Trees can also be utilized to do regression tasks. Lets have a look at the code:
```python
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2)
tree_reg.fit(X, y)
```
The resulting tree would look like this:

**TO INSERT IMAGE**





